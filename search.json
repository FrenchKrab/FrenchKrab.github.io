[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "First blog article test :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexis Plaquet",
    "section": "",
    "text": "I am a second year PhD student at IRIT (Toulouse, France) and I work on interactive approaches for speaker diarization, my goal is to find how to make the most out of human time to improve speaker diarization performance. I work under the supervision of Herv√© Bredin.\n\nRecent work\n\nOn the calibration of powerset speaker diarization models (Alexis Plaquet, Herv√© Bredin) - Proc. INTERSPEECH 2024 (üìÉ üåê üêç)\nPowerset multi-class cross entropy loss for neural speaker diarization (Alexis Plaquet, Herv√© Bredin) - Proc. INTERSPEECH 2023 (üìÉüåê)\nParticipation in JSALT 2023 : xdiar team - confidence measures, calibration, semi-supervised learning\nSemi and self-supervised learning for speaker diarization - 5 months internship from feb-jun 2022, supervised by Herv√© Bredin (üìÉ)\n\n\n\nEducation\n\nEcole Normale Sup√©rieure de Lyon - Master 2 Informatique Fondamentale - 2021-2022\nUniversit√© Lyon 1 Claude Bernard - Master 1 Informatique, rank 2/125 - 2020-2021\nUniversit√© Picardie Jules Verne - Licence d‚Äôinformatique, mention Tr√®s bien - 2017-2020\n\n\n\nOther\n\nüíæ datasets-pyannote : a Python package to download and prepare datasets for pyannote.audio"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "EEND-M2F : Reading notes\n\n\n\n\n\n\npaper_reading\n\n\nspeaker_diarization\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nAlexis Plaquet\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\ntest\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nAlexis Plaquet\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/papers/eend_m2f.html",
    "href": "posts/papers/eend_m2f.html",
    "title": "EEND-M2F : Reading notes",
    "section": "",
    "text": "EEND-M2F: Masked-attention mask transformers for speaker diarization üìÉ ‚Äî by Marc H√§rk√∂nen, Samuel J. Broughton, Lahiru Samarakoon, Interspeech 2024\nThis paper proposes a ‚Äúnovel‚Äù architecture for speaker diarization (actually applies Mask2Former from image segmentation to speaker diarization, which turns out to be a good idea) and claims some very strong results, most notably state-of-the-art diarization error rate on:\n\nAISHELL-4: 13.2 -&gt; 13.2 (nice enough)\nAliMeeting (far): 23.3 -&gt; 13.2 (!)\nAliMeeting (near): 22.59 -&gt; 10.45 (!)\nRAMC: 13.58 -&gt; 11.1 (it seems everyone is stuck at 11.0 or 11.1 nowadays though)\nDIHARD-III : 16.76 -&gt; 16.07 (!)\n\n(as well as some disappointing results on some datasets, until someone fixes the problem by next Interspeech)\nAnd probably the most important part of this paper is that it returns to a pure end-to-end approach to speaker diarization, while being VERY VERY fast with a 5700 real-time factor (158 hours in 100 seconds). Being end-to-end means it requires quite a bit of memory to process long files (= intractable for very long files), but if that ever happens, it‚Äôs always possible to plug these models into an existing ‚Äúhybrid‚Äù pipeline such as pyannote.audio to process the file in chunks.\nI won‚Äôt speed too long on the intuition or results since I‚Äôm mostly interested in explaining how it works. But the main idea is to consider speaker diarization as an object segmentation task. Usually (in vision), this means predicting N masks of the image for N classes of objects, to mask the corresponding objects on the image. Diarization turns out to be a form of object segmentation, where each speaker is an object class, and the image is actually a 1D waveform.\nHere is an attempt at a detailed breakdown of the paper to make it easy to implement (and so that I can read this again when my knowledge on this paper evaporates in one week). I have no interest in analyzing the results, the paper does that better than me. But what the paper doesn‚Äôt have is unlimited space to put wastefully detailed figures &gt;:)\n(Hopefully I‚Äôm not making huge mistakes)"
  },
  {
    "objectID": "posts/papers/eend_m2f.html#feature-extraction",
    "href": "posts/papers/eend_m2f.html#feature-extraction",
    "title": "EEND-M2F : Reading notes",
    "section": "Feature extraction",
    "text": "Feature extraction\nNothing fancy happens there, this is the part in green in the figure.\nFirst we extract D' basic features from a waveform at a certain resolution, this gives us our original input tensor X: (T', D').\nX is then downsampled using convolution to obtain a (T, D')-shaped tensor (in the paper, T=T'/10), that is then passed to (6) conformers. This gives E': (T, D) (\\(\\mathcal{E}\\) in the paper) that has a lower temporal resolution and a higher number of dimensions.\nFinally, we upsample E' with another convolution layer to obtain E: (T', D) that has a higher temporal resolution (same as X) but still more features.\nThe paper chooses D=256 but doesn‚Äôt elaborate on the choice, with Mel-features of size D'=25 of stride 10ms and window size 25ms.\nTo summarize this part:\n\n\n\n\n\nflowchart LR\n    X[\"X\\n(T', D')\"]:::vector --&gt; conv1[conv1]:::module\n    conv1 --&gt; a1[\"(T, D')\"]:::vector\n    a1 --&gt; conf[conformers]:::module\n    conf --&gt; E1[\"E'\\n(T, D)\"]:::vector\n    E1 --&gt; conv2[conv2]:::module\n    conv2 --&gt; E2[\"E\\n(T', D)\"]:::vector\n\n    classDef module fill:#f99,stroke:#333,stroke-width:4px;\n    classDef vector fill:#ddf,stroke:#88d,stroke-width:2px,stroke-dasharray: 10 4;"
  },
  {
    "objectID": "posts/papers/eend_m2f.html#queries-and-transformer-layers",
    "href": "posts/papers/eend_m2f.html#queries-and-transformer-layers",
    "title": "EEND-M2F : Reading notes",
    "section": "Queries and transformer layers",
    "text": "Queries and transformer layers\nThe architecture is centered around ‚Äúqueries‚Äù, called like that because they are passed as queries to the transformer layers of the architecture (surprise). Those transformers are also called ‚Äúquery modules‚Äù since they generate a new query (for the next transformer to ingest).\nThe model contains a learnt initial query tensor Q0: (N, D) (randomly initialized at model creation). The model contains a certain number L of Transformer layers. Each layer receives a query \\(Q_i\\) and outputs a query \\(Q_{i+1}\\), so we will obtain Q1, Q2, ‚Ä¶, QL after each layer. At inference, the speaker diarization output will only be determined from the very last query QL (the others are discarded).\nThe N from the dimension is an hyperparameter that determines the maximum number of supported speakers. The paper chooses N=50 which should be more than enough (and curiously yields better results than 25 or 75). Each of the N queries in the query tensor maps one speaker to some relevant dimensions.\nNow if we want to summarize the process this is what happens:\n\n\n\n\n\nflowchart TD\n    Q0[\"Q0\\n(N, D)\"]:::vector --query--&gt; tr1[Transformer1]:::module\n    M1[\"Mask #1\\n(N, D)\"]:::vector --attn mask--&gt; tr1\n    tr1 --&gt; Q1[\"Q1\\n(N, D)\"]:::vector\n\n    Q1 --query--&gt; tr2[Transformer2]:::module\n    M2[\"Mask #2\\n(N, D)\"]:::vector --attn mask--&gt; tr2\n    tr2 --&gt; Q2[\"Q2\\n(N, D)\"]:::vector\n\n    Q2 -. \"..and so on..\" .-&gt; trL[TransformerL]:::module\n    ML[\"Mask #L\\n(N, D)\"]:::vector --attn mask--&gt; trL\n    trL --&gt; QL[\"QL\\n(N, D)\"]:::vector\n    QL -.Mysterious additional step.-&gt; diar[\"Diarization output\\n(T, S)\"]:::vector\n\n\n    E1[\"E'\\n(T, D)\"]:::vector --key & value--&gt; tr1\n    E1 --key & value--&gt; tr2\n    E1 --key & value--&gt; trL\n\n    classDef module fill:#f99,stroke:#333,stroke-width:4px;\n    classDef vector fill:#ddf,stroke:#88d,stroke-width:2px,stroke-dasharray: 10 4;\n\n\n\n\n\n\nYou can see how we generate new queries after each layer until we‚Äôre left with the final one. And for that each transformer layer uses key=value=E' (downsampled embeddings \\(\\mathcal{E}\\)).\nNow two part of the process are still unexplained:\n\nHow to obtain the attention masks (and why use them) ?\nHow to obtain the final diarization output from the final query ?"
  },
  {
    "objectID": "posts/papers/eend_m2f.html#attention-masks",
    "href": "posts/papers/eend_m2f.html#attention-masks",
    "title": "EEND-M2F : Reading notes",
    "section": "Attention masks",
    "text": "Attention masks\nActually the two last points are obtained from the same process. Let‚Äôs first lay out how it‚Äôs all connected:\n\n\n\n\n\nflowchart TD\n    Q[\"Q\\n(N, D)\"]:::vector --&gt; mlp[MLP]:::module\n    E2[\"E\\n(T, D)\"]:::vector ----&gt; matmul((\"mat\\nmul\")):::op\n    Q --&gt; linear[Linear]:::module\n\n    subgraph mm [MaskModule]\n    mlp --&gt; Q'[\"Q'\\n(N, D)\"]:::vector\n    Q' --transpose--&gt; matmul\n    matmul --&gt; mask1[\"Speaker activation logits\\n(T, N)\"]:::vector\n    end\n    \n    subgraph masksg [Obtaining the attention mask]\n    mask1 --&gt; downsample(\"Downsample\\n(Linear interpolation)\"):::op\n    downsample --&gt; mask2[\"Attention Mask\\n(T', N)\"]:::vector\n    end\n\n    mask1 --&gt; sigmoid(Sigmoid):::op\n\n    subgraph final [\"Obtaining Y from QL (last query)\"]\n    sigmoid --&gt; y1[\"Y&lt;sup&gt;~&lt;/sup&gt; speaker activations\\n(T, N)\"]:::vector\n    linear --&gt; sigmoid2(Sigmoid):::op\n    sigmoid2 --&gt; p[\"p speaker probabilities\\n(N,)\"]:::vector\n\n    p --&gt; filt(\"Keep dimensions of Y&lt;sup&gt;~&lt;/sup&gt; where p &gt; Œ∏\"):::op\n    y1 --&gt; filt\n    filt --&gt; y2[\"Y\\n(T, S)\"]:::vector\n    end\n\n\n    classDef module fill:#f99,stroke:#333,stroke-width:4px;\n    classDef vector fill:#ddf,stroke:#88d,stroke-width:2px,stroke-dasharray: 10 4;\n    classDef op fill:#eee,stroke:#333,stroke-width:4px;\n\n\n\n\n\n\n\nCreating the next attention mask\nThe attention masks are generated using the MaskModule. It take a query tensor Q: (N,D) and the upsampled embeddings E: (T,D), and eventually gives us some speaker activation logits of size (T,N).\nIf we downsample this to (T', N), we get the attention mask. For the motivation I‚Äôll be lazy and just quote the paper which explains very it clearly:\n\nTo explain the motivation behind masked attention, we equate each query with a speaker. Without masked attention, queries need to learn to ignore frames not containing their speaker. Masked attention avoids this issue by design: frames irrelevant to the query are simply masked away, allowing the attention mechanism to focus on relevant acoustic information.\n\n(to be clear, by ‚Äúeach query‚Äù, they mean ‚Äúeach of the N dimensions in the query tensor‚Äù).\n\n\nCreating the final prediction Y from QL\nSpeaker activation logits are also used to obtain the final Y.\nWe take the speaker activation logits (T,N), squash each of them in \\([0,1]\\) with the sigmoid function to obtain the speaker activations \\(\\hat{Y}\\) : (T,N). Since we probably have less than N speakers, we don‚Äôt need that many dimensions to our output.\nTo select which speaker are active, we feed the query to a linear layer that will reduce it from (N,D) to (N,), and after applying the sigmoid function, we get a tensor describing the probabilities in \\([0,1]\\) that there is an active speaker in each of the (N,) channels.\nWe construct the final output by selecting a hyperparameter \\theta and keeping only speaker dimensions where the speaker probabilities p are \\(&gt; \\theta\\). In other words, in pytorch language Y = spk_act[:, p &gt; theta]. The paper uses \\(\\theta = 0.8\\) but it‚Äôs unclear how and when it is chosen."
  },
  {
    "objectID": "posts/papers/eend_m2f.html#permutation-invariance",
    "href": "posts/papers/eend_m2f.html#permutation-invariance",
    "title": "EEND-M2F : Reading notes",
    "section": "Permutation invariance",
    "text": "Permutation invariance\nDuring training, the prediction \\(\\tilde{Y}\\) (T,N) does not have the same shape as the reference (T,S). And if we did things right, N&gt;&gt;S so we have to select what are the S speakers from \\(\\tilde{Y}\\) we want to compute the loss on (and align their identities with the reference). To do that, the paper uses hungarian matching.\nThe cost function it uses for hungarian matching is:\n\\(\\mathcal{L} = \\lambda_{dia} \\cdot \\mathcal{L}_{matchdia} + \\lambda_{dice} \\cdot \\mathcal{L}_{matchdice} + \\lambda_{cls} \\cdot \\mathcal{L}_{matchcls}\\)\nRemember, this does not imply any backpropagation or learning, this is only to decide the best S speakers to select among the N possible speakers.\n\\(\\mathcal{L}_{matchdia}\\) is the binary cross entropy loss for speaker diarization, averaged over time.\n\\(\\mathcal{L}_{matchdice}\\) is the sum of Dice loss, explained in next part.\n\\(\\mathcal{L}_{matchcls} = - \\sum^{S}_{i=1} p_{\\Phi(i)}\\) is the classification loss, where \\(\\Phi\\) is the considered permutation. Note that we depend on p for this, not \\(\\tilde{Y}\\) ! This classification loss encourages selection of speakers that are predicted active by the part of the network responsible for active speaker detection (the ‚ÄòLinear‚Äô in the figure above).\nFor example if we have only one frame with p=[0.01, 0.1, 0.9, 0.8] and ref=[1,1], we get losses\n\n\n\n\\(\\Phi\\)‚Äôs indices\nclassification loss\n\n\n\n\n{0,1}\n-0.11\n\n\n{0,2}\n-0.91\n\n\n{2,3}\n-1.17\n\n\n\nThe paper states this is necessary to avoid duplicate predictions during inference, and honestly I can‚Äôt figure out why‚Ä¶"
  },
  {
    "objectID": "posts/papers/eend_m2f.html#training-loss",
    "href": "posts/papers/eend_m2f.html#training-loss",
    "title": "EEND-M2F : Reading notes",
    "section": "Training loss",
    "text": "Training loss\nUsing the previously found best permutation \\(\\Phi*\\) and \\(\\tilde{Y}'\\) now accordingly aligned and (T,S)-shaped. The permutation loss is:\n\\(\\mathcal{L} = \\lambda_{dia} \\cdot \\mathcal{L}_{dia} + \\lambda_{dice} \\cdot \\mathcal{L}_{dice} + \\lambda_{cls} \\cdot \\mathcal{L}_{cls}\\)\nWith:\n\ndiarization loss\nThe good old binary cross entropy, I don‚Äôt feel like I have to explain it again. \\(\\mathcal{L}_{dia} = BCE(\\tilde{Y}', ref)\\).\nThe paper mentions only computing it on active speakers, but I think that‚Äôs what happens naturally if you use the aligned \\(\\tilde{Y}'\\) instead of \\(\\tilde{Y}\\).\n\n\ndice loss (üìÉ)\nScale invariant loss that affect all active speaker equally, regardless of their how active they are.\nI‚Äôm too lazy to property understand this for now, sorry ‚Ä¶\n\n\nclassification loss\nBinary cross entropy between the speaker probabilities p: (N,) and the (aligned) reference 0/1 speaker presence.\nTo explain it in pytorch pseudocode:\n# p: (N,)\n# phi: (N,) permutation where the S first indices are the S speakers, aligned to ref via hungarian matching\n# (S = number of active speakers in the reference)\nNEG_CLS_FACTOR = 0.2\n\np2 = p[phi] # apply the hungarian mapping to p\n\n# create ref_p with S ones and (N-S) zeros\nref_p = torch.zeros(N)\nref_p[:S] = 1\n\n# less weight to negative classes\nw = torch.ones(N) * NEG_CLS_FACTOR\nw[:S] = 1.0\n\ncls_loss = BCE(input=p2, target=ref_p, weight=w)\nBasically, we penalize the network if it predicts too few or too many speakers. The paper also weights negative classes less with an arbitrary weight of 0.2. The idea is to lower the impact of the overwhelwing majority of negative classes (if N=50, we have more than 40 negative classes most of the time).\n\n\nAbout the factors \\(\\lambda\\) ‚Ä¶\nI don‚Äôt know how they were chosen, I don‚Äôt think they are mentionned in the paper. The values used are:\n\n\\(\\lambda_{dia} = 5\\)\n\\(\\lambda_{dice} = 5\\)\n\\(\\lambda_{cls} = 2\\)\n\nWhich must have been determined using some hyperparameter search to minimize the DER or the training speed. I would be curious to see the influence of these weighting factors on the diarization output / error type distribution / training speed.\nAlso they seem to be shared for hungarian matching and training loss."
  },
  {
    "objectID": "posts/papers/eend_m2f.html#deep-supervision",
    "href": "posts/papers/eend_m2f.html#deep-supervision",
    "title": "EEND-M2F : Reading notes",
    "section": "Deep supervision",
    "text": "Deep supervision\nTo help the model and make sure the intermediate predictions of the model ‚Äúmake sense‚Äù (all the Qi query tensors), the paper uses deep supervision.\nWithout deep supervision, we would simply compute the speaker activations \\(\\tilde{Y}\\) and p from the final query QL, apply the losses on them and backpropagate.\nWith deep supervision, we compute \\(\\tilde{Y}\\) and p from each intermediate query Qi and apply hungarian matching + loss + backpropagation on each of them (even the first one Q0) !"
  }
]